{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd09342c2e715601ee7d69df8a755bd808fe01148d99e2b8eb23d76eb9a3fd9f478",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Table of Contents\n",
    "[Section 1: Introduction]\n",
    "    -[Section 1.i: Historical Context](#section-1i-hisorical-context)\n",
    "    -[Section 1.ii: Scope of Problem](#section-1ii-scope-of-problem)\n",
    "[Section 2: Methodology](#section-2-methodology)\n",
    "    -[Section 2.i: Processing](#section-2i-processing)\n",
    "    -[Section 2.ii: Preprocessing](#section-2i-processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-1f5e0d95f412>, line 1)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-1f5e0d95f412>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    https://stackoverflow.com/questions/11948245/markdown-to-create-pages-and-table-of-contents/33433098#33433098\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "https://stackoverflow.com/questions/11948245/markdown-to-create-pages-and-table-of-contents/33433098#33433098\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Section 1: Introduction"
   ]
  },
  {
   "source": [
    "### Section 1.i: Historical Context\n",
    "\n",
    "At the inception of the Ontario Birth Study in 2013, clinical data was stored on a Medidata Rave database (https://www.medidata.com/en/products/edc/) which was housed at the Applied Health Research Centre (AHRC; https://www.stmichaelshospital.com/research/ahrc/) at St. Michaelâ€™s Hospital. Medidata Rave was used as it was believed that the Ontario Birth Study data would be associated with the Ontario Health Study (https://www.ontariohealthstudy.ca/). Since the Ontario Birth Study is no longer associated with the Ontario Health Study and there were significant limitations with the Medidata Rave database, the Ontario Birth Study decided to move the clinical database from Medidata Rave to REDCap in late 2019. Both the Medidata Rave and REDCap database were developed by AHRC; however, the data transfer was done independently of ARHC. \n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Section 1.ii: Scope of Problem\n",
    "\n",
    "This notebook outlines the data migration process from Medidata Rave to REDCap completed in May 2020. The process outlined attempts to maximize data fidelity with minimal data cleaning. It was anticipated that the data cleaning proces would be done at a later date. Therefore, for the purposes of this project, it was assumed that all Medidata RAVE data received from AHRC was correct (i.e. no checking between label and coded data).\n",
    "\n",
    "Although both the Medidata Rave and REDCap database were created by AHRC, there were differences in format and the numeric coding of variables. The RAVE data was given to the Ontario Birth Study in a wide format while the REDCap data needs to be in a long format to successfully import. Some columns, specifically date columns, forced the person entering the data to input a value even if one was not available. These instances had to be reinterpreted so the REDCap data reflected the intentions of the source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Section 2: Methodology\n",
    "Broad overview of how the data was treated and checked"
   ]
  },
  {
   "source": [
    "### Section 2.i: Processing\n",
    "The RAVE database captures data in a wide format whereas the REDCap data is a long/narrow format. The `RedcapConv` class from `obs_clinic_migration` was created to aid in the conversion. The intializing of `RedcapConv` takes the following parameters: `ravestub_redcap_dict`, `stub_repeat`, `master_df`, `redcap_data_dict`, and, `recode_long`.\n",
    "\n",
    "The `ravestub_redcap_dict` maps the name of the variable in REDCap to the associated variable in RAVE.\n",
    "\n",
    "The `stub_repeat` parameter is the number of occurences in the RAVE dataset. For example if there is 'parameter_1' and 'parameter_2', the stub_repeat would be set to 2.\n",
    "\n",
    "The `master_df` is the reference data frame that is being convert - in this case it is the RAVE dataframe which will be converted to the REDCap format.\n",
    "\n",
    "redcap_data_dict\n",
    "\n",
    "The `recode_long` Recode RAVE long dataframe based on REDcap data dictionar;            will change the output value based on the REDCAP DICITONARY\n",
    "\n",
    "\n",
    "The function works by identifying the columns of interest in the RAVE clinic database. \n",
    "\n",
    "For example, suppose we are interested in the RAVE columns `rave_column_a` and `rave_column_b`. Two RAVE columns are availble in the RAVE data set: one with the data uncoded (i.e. represented as a string), another representing the data numerically coded. In the RAVE data set, the column representing the data numerically coded ended in 'STD' (e.g. `rave_parameter_a_STD`). \n",
    "\n",
    "\n",
    "Table 1. Example raw data from RAVE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "| Subject | rave_column_a | rave_column_a_STD | rave_column_b | rave_column_b_STD |\n",
    "|---------|---------------|-------------------|---------------|-------------------|\n",
    "| 100001  | Yes           | 1                 | Male          | 1                 |\n",
    "| 100002  | No            | 0                 | Male          | 1                 |\n",
    "| 100003  | Yes           | 1                 | Female        | 0                 |"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "The uncoded column is referenced as the key in a dictionary (i.e. `ravestub_redcap_dict`) - the coded column is ignored. The corresponding dictionary value represents the name of the REDCap column. The dictionary is used as an argument for the `RedcapConv` class during initilization. With the dictionary\n",
    "```python\n",
    "{\n",
    "        `rave_column_a`: `redcap_column_a`,\n",
    "        `rave_column_b`: `redcap_column_b`\n",
    "}\n",
    "```\n",
    "\n",
    "Table 1 will be converted into Table 2.\n",
    "Table 2."
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "| obs_id | redcap_column_a | redcap_column_b |\n",
    "|---------|-----------------|-----------------|\n",
    "| 100001  | Yes             | Male            |\n",
    "| 100002  | No              | Male            |\n",
    "| 100003  | Yes             | Female          |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If `recode_long` is set to `True`, then the initilization method will reference the column name in the REDCap data dictionary (i.e. `redcap_data_dict` parameter) and make the changes accordingly. For example, if the REDCap data dictionary listed the coding of `redcap_column_a` as '1, No | 2, Yes' and `redcap_column_b` as '1, Male | 2, Female | 3, Unknown', the table would result in Table 3.\n",
    "\n",
    "Table 3."
   ]
  },
  {
   "source": [
    "| obs_id | redcap_column_a | redcap_column_b |\n",
    "|---------|-----------------|-----------------|\n",
    "| 100001  | 2               | 1               |\n",
    "| 100002  | 1               | 1               |\n",
    "| 100003  | 2               | 2               |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The aforementioned example only works with columns were there is are no repeat instances (i.e. 'stub_repeat = 0'). Since the RAVE data is captured in a wide format, it needs to be converted to a long format before being imported into REDCap. For example, suppose the columns of interest are 'rave_column_a_1', 'rave_column_a_2', 'rave_column_b_1', and 'rave_column_b_2' where 'rave_column_a_1' and 'rave_column_a_2' are different iterations of the same parameter (similarly for 'rave_column_b_1' and 'rave_column_b_2'). \n",
    "\n",
    "RedcapConv has to manually enter number of instances > first surgery column is inconsistently named with the rest of the columns\n",
    "\n",
    "Table 4."
   ]
  },
  {
   "source": [
    "| Subject | rave_column_a_1 | rave_column_a_1_STD | rave_column_a_2 | rave_column_a_2_STD | rave_column_b_1 | rave_column_b_1_STD | rave_column_b_2 | rave_column_b_2_STD |\n",
    "|-|-|-|-|-|-|-|-|-|\n",
    "| 100001 | Yes | 1 | No | 0 | Yes | 1 | No | 0 |\n",
    "| 100002 | No | 0 | No | 0 | Yes | 1 | Yes | 1 |\n",
    "| 100003 | Yes | 1 |  |  | Yes | 1 |  |  |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "When 'stub_repeat' is set appropriately, Table 4 is converted to Table 5\n",
    "\n",
    "Table 5."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "| Subject | redcap_repeat_instance | rave_column_a | rave_column_b |\n",
    "|-|-|-|-|\n",
    "| 100001 | 1 | Yes | Yes |\n",
    "| 100002 | 1 | No | Yes |\n",
    "| 100003 | 1 | Yes | Yes |\n",
    "| 100001 | 2 | No | No |\n",
    "| 100002 | 2 | No | Yes |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Section 2.ii: Preprocessing\n",
    "The RAVE columns are mapped one-to-one to REDCap columns.\n",
    "\n",
    "rave_date_unknown\n",
    "Add 'date unavailabe' column to RAVE dataframe.\n",
    "    \n",
    "    The new REDCap database has a column which indicates if date was available \n",
    "    for some variables. An equivalent column may not exist in the RAVE \n",
    "    database.\n",
    "\n",
    "\n",
    "create_specify_col\n",
    "Add 'please specify' column to RAVE dataframe\n",
    "    \n",
    "    The new REDCap database has a separate column for 'please specify'. In the\n",
    "    RAVE database 'please specify' is intermingled in a column with the \n",
    "    associated labelled data. This function separates the 'please specify' \n",
    "    answer into a unique column based on the coded column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Section 2.ii: Postprocessing\n",
    "There are instances when the processing is insufficient. Two additional methods were created to aid in migration.\n",
    "\n",
    "#### `change_str` method\n",
    "There were some instances where there were spelling variations between the RAVE database and the REDCap dictionary which arose as part of the database creation process. For example, if the REDCap data dictionary for column 'redcap_column_a' was '1, No | 2, Yes', the values in Table 6 would not be converted as part of the normal Processing process (Section 2.i) since the strings are different. If no changes were made, the associated values would remain as strings, not converted to integers, and there would be issues during the import process.\n",
    "\n",
    "Table 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "| obs_id | redcap_column_a |\n",
    "|---------|-----------------|\n",
    "| 100001  | YES             |\n",
    "| 100002  | NO              |\n",
    "| 100003  | YES             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Once these discrepancies were identified, the 'change_str' method was used to alter the spelling of the values so they can be coded correctly. Passing the following argument to the 'change_str' can convert Table 6 to integers which can be imported without issues.\n",
    "```\n",
    "{'redcap_column_a': \n",
    "    {\n",
    "        'YES' : 'Yes',\n",
    "        'NO': 'No'   \n",
    "    }\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### `remove_na` method\n",
    "In some instances, converting from the wide format in RAVE to the long format in REDCap resulted in additional rows with no data. The `remove_na` method removed these rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Section 2.iii: Data Verification\n",
    "Three different processes were employed to ensure the fidelity of the data: code testing, double data entry, and the use of REDCap's internal data processing tools\n",
    "To ensure the fidelity of the data, 3 different processes were employed\n",
    "\n",
    "\n",
    "#### Code Quality Assurance\n",
    "Code used to create the output files were functionally tested using `pytest` with the results available [here].\n",
    "\n",
    "#### Data Verification\n",
    "Double data entry process was used for [data verification](https://en.wikipedia.org/wiki/Data_verification) as suggested by technical articles[^1] and academic literature[^11][^12].\n",
    "\n",
    "The data from 40 subjects was directly entered into both the RAVE and REDCap database. These 40 subjects were selected to maximize the number of columns covered in the RAVE data set. The first subject that was selected had the most amount of non-empty columns in the RAVE data set. The second subject selected had the most amount of non-empty columns after ignoring the non-empty columns the first subject had. The third subject selected had the most amount of non-empty columns after ignoring the non-empty columns the first and second subject had. This process continued until 40 subjects were selected and is outlined in `double_data_entry_subjects.py`. \n",
    "\n",
    "Once the RAVE data was converted into a format suitable to import into REDCap, it was then compared to the data directly entered into REDCap. If any discrepancies existed, these were rectified before the import process.\n",
    "\n",
    "#### REDCap tools\n",
    "The REDCap GUI provides the ability to visualize any discrepancies with the imported csv files (Figure 1). Any data that does not conform to the REDCap configuration will be flagged and the data will not be imported until the discrepancies have been resolved or the associated record is removed.\n"
   ]
  },
  {
   "source": [
    "<img src=\"figures/data-imports-7.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Figure 1. Example of REDCap data import quality check\n",
    "\n",
    "https://docs.redcap.qmul.ac.uk/adding-data/import-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Section 2.iv: Prepare for Import and Save\n",
    "\n",
    "For example,\n",
    "The `prep_imp` method adds three different columns 'redcap_event_name', 'redcap_repeat_instrument', and the name of a completion column (i.e. `redcap_sheet_name_complete`). These values are not reviewed as part of the data verification process since they are reviewed during REDCap's \n",
    "import process. Finally, the data is saved locally and manually imported using REDCap's GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Section 2.v: Overall check for missing columns\n",
    "\n",
    "The data columns that were created as part of this process were compared to the data columns in REDCap (see SECTION X). This process was used to ensure all relevant columns were accounted - there were some columns in the REDCap project which were not in the RAVE database (e.g. Diabetes in Pregnancy) which were ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Section 3: Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Preprocessing\n",
    "```\n",
    "rave_clinic = rave_date_unknown(rave_clinic, 'SURG_PROCS_AFTER_BASLIN_NY_', 'Yes', 'PROCEDURE_DT_', 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Processing\n",
    "```\n",
    "ante_sur_dict = {\n",
    "    'SURG_PROCS_AFTER_BASLIN_NY_': 'ante_sx_yn',\n",
    "    'SURG_PROC_PERF_DURN_PREG_': 'ante_sx_class',\n",
    "    'SPECIFY_PROCEDURE_': 'ante_sx_spec',\n",
    "    'PROCEDURE_DT_DD_': 'ante_sx_day',\n",
    "    'PROCEDURE_DT_MM_': 'ante_sx_month',\n",
    "    'PROCEDURE_DT_YYYY_': 'ante_sx_year',\n",
    "    'INDICATION_': 'ante_sx_indication',\n",
    "    'COMMENT1_': 'ante_sx_comment',\n",
    "    'PROCEDURE_DT_yn_date_': 'ante_sx_date_yn'     \n",
    "}   \n",
    "ante_sur = RedcapConv(ante_sur_dict, 2, master_df = rave_clinic)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Postprocessing\n",
    "```\n",
    "ante_sur.change_str(\n",
    "    {'ante_sx_class': {'Other, specify': 'Other'},}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data Verification\n",
    "```\n",
    "ante_sur.compare_conv_dde(redcap_clinic)\n",
    "```"
   ]
  },
  {
   "source": [
    "#### Prepare for import and save\n",
    "```\n",
    "ante_sur.prep_imp('antenatal_arm_1', 'surgical_procedures_performed_during_pregnancy_complete', 'surgical_procedures_performed_during_pregnancy')    \n",
    "ante_sur.data.to_csv('../data/processed/ante_sur.csv', index = False)\n",
    "```"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[^1]: https://www.computerworld.com/article/2553608/avoiding-data-migration-delays.html\n",
    "[^2]: https://academic.oup.com/jamia/article/9/6/600/1036696\n",
    "[^11]: https://pubmed.ncbi.nlm.nih.gov/8235176/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "https://pubmed.ncbi.nlm.nih.gov/23646084/\n",
    "\n",
    "https://pubmed.ncbi.nlm.nih.gov/8235176/\n",
    "https://influentialpoints.com/Training/Data_verification_Use_and_misuse.htm"
   ]
  }
 ]
}